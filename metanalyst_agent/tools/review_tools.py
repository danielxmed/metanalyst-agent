"""
Ferramentas de revis√£o de qualidade para valida√ß√£o de relat√≥rios e conformidade PRISMA.
"""

from typing import List, Dict, Any, Optional
from langchain_core.tools import tool
from datetime import datetime

from ..config.settings import settings


@tool
def assess_report_quality(
    report_sections: Dict[str, str],
    meta_analysis_results: Dict[str, Any]
) -> Dict[str, Any]:
    """
    Avalia a qualidade geral do relat√≥rio de meta-an√°lise.
    
    Args:
        report_sections: Se√ß√µes do relat√≥rio
        meta_analysis_results: Resultados da meta-an√°lise
    
    Returns:
        Avalia√ß√£o de qualidade com score e recomenda√ß√µes
    """
    try:
        quality_assessment = {
            "overall_score": 0,
            "section_scores": {},
            "recommendations": [],
            "strengths": [],
            "weaknesses": [],
            "compliance_issues": []
        }
        
        # Avaliar cada se√ß√£o
        max_score = 100
        section_weights = {
            "abstract": 15,
            "introduction": 10,
            "methods": 25,
            "results": 25,
            "discussion": 15,
            "conclusion": 10
        }
        
        total_score = 0
        
        for section, content in report_sections.items():
            section_score = evaluate_section_quality(section, content, meta_analysis_results)
            quality_assessment["section_scores"][section] = section_score
            
            weight = section_weights.get(section, 10)
            total_score += (section_score / 100) * weight
        
        quality_assessment["overall_score"] = min(100, total_score)
        
        # Avaliar adequa√ß√£o estat√≠stica
        stats_assessment = assess_statistical_adequacy(meta_analysis_results)
        quality_assessment.update(stats_assessment)
        
        # Gerar recomenda√ß√µes baseadas no score
        if quality_assessment["overall_score"] >= 90:
            quality_assessment["quality_level"] = "Excelente"
            quality_assessment["recommendations"].append("Relat√≥rio de alta qualidade, pronto para publica√ß√£o.")
        elif quality_assessment["overall_score"] >= 75:
            quality_assessment["quality_level"] = "Boa"
            quality_assessment["recommendations"].append("Relat√≥rio de boa qualidade com pequenos ajustes necess√°rios.")
        elif quality_assessment["overall_score"] >= 60:
            quality_assessment["quality_level"] = "Adequada"
            quality_assessment["recommendations"].append("Relat√≥rio adequado, mas requer revis√µes substanciais.")
        else:
            quality_assessment["quality_level"] = "Inadequada"
            quality_assessment["recommendations"].append("Relat√≥rio requer revis√£o completa antes da finaliza√ß√£o.")
        
        quality_assessment["assessment_date"] = datetime.now().isoformat()
        
        return quality_assessment
        
    except Exception as e:
        return {
            "error": f"Erro na avalia√ß√£o de qualidade: {str(e)}",
            "assessment_date": datetime.now().isoformat()
        }


def evaluate_section_quality(section: str, content: str, meta_results: Dict[str, Any]) -> int:
    """Avalia qualidade de uma se√ß√£o espec√≠fica."""
    score = 0
    
    if section == "abstract":
        # Verificar elementos essenciais do abstract
        required_elements = ["objetivo", "m√©todos", "resultados", "conclus√£o"]
        for element in required_elements:
            if element.lower() in content.lower():
                score += 25
    
    elif section == "methods":
        # Verificar elementos metodol√≥gicos
        method_elements = ["prisma", "crit√©rios", "busca", "sele√ß√£o", "extra√ß√£o", "an√°lise"]
        for element in method_elements:
            if element.lower() in content.lower():
                score += 15
    
    elif section == "results":
        # Verificar presen√ßa de resultados estat√≠sticos
        if meta_results.get("pooled_effect_size") is not None:
            score += 30
        if meta_results.get("confidence_interval"):
            score += 25
        if meta_results.get("heterogeneity"):
            score += 25
        if "forest plot" in content.lower() or "gr√°fico" in content.lower():
            score += 20
    
    elif section == "discussion":
        # Verificar elementos de discuss√£o
        discussion_elements = ["limita√ß√µes", "implica√ß√µes", "heterogeneidade", "vi√©s"]
        for element in discussion_elements:
            if element.lower() in content.lower():
                score += 25
    
    # Verifica√ß√µes gerais para todas as se√ß√µes
    if len(content) > 200:  # Conte√∫do substancial
        score += 10
    if "estudo" in content.lower() or "study" in content.lower():
        score += 10
    
    return min(100, score)


def assess_statistical_adequacy(meta_results: Dict[str, Any]) -> Dict[str, Any]:
    """Avalia adequa√ß√£o estat√≠stica da meta-an√°lise."""
    assessment = {
        "statistical_adequacy": "adequate",
        "statistical_issues": [],
        "statistical_strengths": []
    }
    
    # Verificar n√∫mero de estudos
    n_studies = meta_results.get("studies_included", 0)
    if n_studies < settings.min_studies_for_analysis:
        assessment["statistical_issues"].append(f"N√∫mero insuficiente de estudos ({n_studies})")
        assessment["statistical_adequacy"] = "inadequate"
    elif n_studies >= 10:
        assessment["statistical_strengths"].append("N√∫mero adequado de estudos para meta-an√°lise robusta")
    
    # Verificar heterogeneidade
    i_squared = meta_results.get("heterogeneity", {}).get("I_squared", 0)
    if i_squared > 75:
        assessment["statistical_issues"].append(f"Heterogeneidade muito alta (I¬≤ = {i_squared:.1f}%)")
    elif i_squared <= 25:
        assessment["statistical_strengths"].append("Baixa heterogeneidade entre estudos")
    
    # Verificar intervalos de confian√ßa
    ci = meta_results.get("confidence_interval", [])
    if len(ci) == 2:
        ci_width = abs(ci[1] - ci[0])
        if ci_width > 2:
            assessment["statistical_issues"].append("Intervalo de confian√ßa muito amplo")
        else:
            assessment["statistical_strengths"].append("Intervalo de confian√ßa adequado")
    
    # Verificar signific√¢ncia
    p_value = meta_results.get("p_value", 1)
    if p_value < 0.001:
        assessment["statistical_strengths"].append("Resultado altamente significativo")
    elif p_value < 0.05:
        assessment["statistical_strengths"].append("Resultado estatisticamente significativo")
    
    return assessment


@tool
def check_prisma_compliance(
    report_sections: Dict[str, str],
    meta_analysis_results: Dict[str, Any]
) -> Dict[str, Any]:
    """
    Verifica conformidade com diretrizes PRISMA.
    
    Args:
        report_sections: Se√ß√µes do relat√≥rio
        meta_analysis_results: Resultados da meta-an√°lise
    
    Returns:
        Avalia√ß√£o de conformidade PRISMA
    """
    try:
        prisma_checklist = {
            "title": False,
            "abstract_structured": False,
            "introduction_rationale": False,
            "methods_protocol": False,
            "methods_eligibility": False,
            "methods_search": False,
            "methods_selection": False,
            "methods_data_extraction": False,
            "methods_quality_assessment": False,
            "methods_statistical_analysis": False,
            "results_study_selection": False,
            "results_study_characteristics": False,
            "results_risk_of_bias": False,
            "results_individual_studies": False,
            "results_synthesis": False,
            "results_additional_analysis": False,
            "discussion_summary": False,
            "discussion_limitations": False,
            "discussion_conclusions": False,
            "funding": False
        }
        
        # Verificar elementos no abstract
        abstract = report_sections.get("abstract", "")
        if all(word in abstract.lower() for word in ["objetivo", "m√©todos", "resultados", "conclus√£o"]):
            prisma_checklist["abstract_structured"] = True
        
        # Verificar m√©todos
        methods = report_sections.get("methods", "")
        if "prisma" in methods.lower():
            prisma_checklist["methods_protocol"] = True
        if "crit√©rios" in methods.lower() and "inclus√£o" in methods.lower():
            prisma_checklist["methods_eligibility"] = True
        if "busca" in methods.lower() and "base" in methods.lower():
            prisma_checklist["methods_search"] = True
        if "sele√ß√£o" in methods.lower() and "revisor" in methods.lower():
            prisma_checklist["methods_selection"] = True
        if "extra√ß√£o" in methods.lower():
            prisma_checklist["methods_data_extraction"] = True
        if "qualidade" in methods.lower():
            prisma_checklist["methods_quality_assessment"] = True
        if "an√°lise" in methods.lower() and "estat√≠stica" in methods.lower():
            prisma_checklist["methods_statistical_analysis"] = True
        
        # Verificar resultados
        results = report_sections.get("results", "")
        if "estudos" in results.lower() and "inclu√≠dos" in results.lower():
            prisma_checklist["results_study_selection"] = True
        if "caracter√≠sticas" in results.lower():
            prisma_checklist["results_study_characteristics"] = True
        if meta_analysis_results.get("individual_studies"):
            prisma_checklist["results_individual_studies"] = True
        if meta_analysis_results.get("pooled_effect_size") is not None:
            prisma_checklist["results_synthesis"] = True
        if meta_analysis_results.get("heterogeneity"):
            prisma_checklist["results_additional_analysis"] = True
        
        # Verificar discuss√£o
        discussion = report_sections.get("discussion", "")
        if "achados" in discussion.lower() or "resultados" in discussion.lower():
            prisma_checklist["discussion_summary"] = True
        if "limita√ß√µes" in discussion.lower():
            prisma_checklist["discussion_limitations"] = True
        if "conclus√£o" in discussion.lower() or "implica√ß√µes" in discussion.lower():
            prisma_checklist["discussion_conclusions"] = True
        
        # Calcular score de conformidade
        total_items = len(prisma_checklist)
        compliant_items = sum(prisma_checklist.values())
        compliance_score = (compliant_items / total_items) * 100
        
        # Identificar itens em falta
        missing_items = [item for item, compliant in prisma_checklist.items() if not compliant]
        
        return {
            "compliance_score": compliance_score,
            "compliant_items": compliant_items,
            "total_items": total_items,
            "compliance_level": get_compliance_level(compliance_score),
            "missing_items": missing_items,
            "prisma_checklist": prisma_checklist,
            "recommendations": generate_prisma_recommendations(missing_items),
            "checked_at": datetime.now().isoformat()
        }
        
    except Exception as e:
        return {
            "error": f"Erro na verifica√ß√£o PRISMA: {str(e)}",
            "checked_at": datetime.now().isoformat()
        }


def get_compliance_level(score: float) -> str:
    """Determina n√≠vel de conformidade PRISMA."""
    if score >= 90:
        return "Excelente"
    elif score >= 80:
        return "Boa"
    elif score >= 70:
        return "Adequada"
    elif score >= 60:
        return "Limitada"
    else:
        return "Inadequada"


def generate_prisma_recommendations(missing_items: List[str]) -> List[str]:
    """Gera recomenda√ß√µes baseadas em itens PRISMA em falta."""
    recommendations = []
    
    critical_items = [
        "methods_eligibility",
        "methods_search", 
        "methods_statistical_analysis",
        "results_synthesis",
        "discussion_limitations"
    ]
    
    for item in missing_items:
        if item in critical_items:
            if item == "methods_eligibility":
                recommendations.append("Adicionar crit√©rios de elegibilidade claros e espec√≠ficos")
            elif item == "methods_search":
                recommendations.append("Detalhar estrat√©gia de busca e bases de dados utilizadas")
            elif item == "methods_statistical_analysis":
                recommendations.append("Explicar m√©todos estat√≠sticos utilizados na meta-an√°lise")
            elif item == "results_synthesis":
                recommendations.append("Apresentar resultados da s√≠ntese quantitativa (meta-an√°lise)")
            elif item == "discussion_limitations":
                recommendations.append("Discutir limita√ß√µes do estudo e da evid√™ncia")
    
    if len(missing_items) > 10:
        recommendations.append("Revisar estrutura geral do relat√≥rio para melhor ader√™ncia √†s diretrizes PRISMA")
    
    return recommendations


@tool
def validate_statistics(
    meta_analysis_results: Dict[str, Any]
) -> Dict[str, Any]:
    """
    Valida a adequa√ß√£o dos c√°lculos estat√≠sticos.
    
    Args:
        meta_analysis_results: Resultados da meta-an√°lise
    
    Returns:
        Valida√ß√£o estat√≠stica com alertas e recomenda√ß√µes
    """
    try:
        validation = {
            "is_valid": True,
            "warnings": [],
            "errors": [],
            "recommendations": [],
            "statistical_power": "adequate"
        }
        
        # Validar n√∫mero de estudos
        n_studies = meta_analysis_results.get("studies_included", 0)
        if n_studies < 2:
            validation["errors"].append("N√∫mero insuficiente de estudos para meta-an√°lise")
            validation["is_valid"] = False
        elif n_studies < 5:
            validation["warnings"].append("N√∫mero limitado de estudos pode afetar a robustez dos resultados")
            validation["statistical_power"] = "limited"
        
        # Validar intervalos de confian√ßa
        ci = meta_analysis_results.get("confidence_interval", [])
        effect_size = meta_analysis_results.get("pooled_effect_size", 0)
        
        if len(ci) == 2:
            if ci[0] > ci[1]:
                validation["errors"].append("Intervalo de confian√ßa inv√°lido (limite inferior > superior)")
                validation["is_valid"] = False
            
            # Verificar se effect size est√° dentro do CI
            if not (ci[0] <= effect_size <= ci[1]):
                validation["errors"].append("Effect size fora do intervalo de confian√ßa")
                validation["is_valid"] = False
            
            # Verificar largura do CI
            ci_width = abs(ci[1] - ci[0])
            if ci_width > 3:
                validation["warnings"].append("Intervalo de confian√ßa muito amplo indica baixa precis√£o")
        
        # Validar heterogeneidade
        heterogeneity = meta_analysis_results.get("heterogeneity", {})
        i_squared = heterogeneity.get("I_squared", 0)
        
        if i_squared > 90:
            validation["warnings"].append("Heterogeneidade extremamente alta questiona adequa√ß√£o da meta-an√°lise")
        elif i_squared > 75:
            validation["warnings"].append("Heterogeneidade alta requer investiga√ß√£o de subgrupos")
        
        # Validar p-value
        p_value = meta_analysis_results.get("p_value", 1)
        if p_value < 0 or p_value > 1:
            validation["errors"].append("P-value inv√°lido (deve estar entre 0 e 1)")
            validation["is_valid"] = False
        
        # Validar model choice
        model = meta_analysis_results.get("model_used", "")
        if i_squared > 25 and model == "fixed_effects":
            validation["recommendations"].append("Considerar modelo de efeitos aleat√≥rios devido √† heterogeneidade")
        
        # Validar tamanho da amostra total
        total_participants = meta_analysis_results.get("total_participants", 0)
        if total_participants < 100:
            validation["warnings"].append("Tamanho amostral total pequeno limita generaliza√ß√£o")
        
        return {
            **validation,
            "validated_at": datetime.now().isoformat()
        }
        
    except Exception as e:
        return {
            "is_valid": False,
            "error": f"Erro na valida√ß√£o estat√≠stica: {str(e)}",
            "validated_at": datetime.now().isoformat()
        }


@tool
def generate_quality_report(
    quality_assessment: Dict[str, Any],
    prisma_compliance: Dict[str, Any],
    statistical_validation: Dict[str, Any]
) -> str:
    """
    Gera relat√≥rio consolidado de qualidade.
    
    Args:
        quality_assessment: Avalia√ß√£o de qualidade
        prisma_compliance: Conformidade PRISMA
        statistical_validation: Valida√ß√£o estat√≠stica
    
    Returns:
        Relat√≥rio de qualidade formatado
    """
    try:
        report = f"""
## RELAT√ìRIO DE QUALIDADE - META-AN√ÅLISE

### Resumo Executivo
- **Qualidade Geral:** {quality_assessment.get('quality_level', 'N/A')} (Score: {quality_assessment.get('overall_score', 0):.1f}/100)
- **Conformidade PRISMA:** {prisma_compliance.get('compliance_level', 'N/A')} ({prisma_compliance.get('compliance_score', 0):.1f}%)
- **Valida√ß√£o Estat√≠stica:** {'‚úÖ V√°lida' if statistical_validation.get('is_valid', False) else '‚ùå Inv√°lida'}

### Avalia√ß√£o por Se√ß√µes
"""
        
        # Scores por se√ß√£o
        for section, score in quality_assessment.get("section_scores", {}).items():
            report += f"- **{section.title()}:** {score}/100\n"
        
        report += f"""

### Conformidade PRISMA
- **Itens Atendidos:** {prisma_compliance.get('compliant_items', 0)}/{prisma_compliance.get('total_items', 0)}
- **Itens em Falta:** {len(prisma_compliance.get('missing_items', []))}

### Valida√ß√£o Estat√≠stica
"""
        
        if statistical_validation.get("errors"):
            report += "**‚ùå Erros Cr√≠ticos:**\n"
            for error in statistical_validation["errors"]:
                report += f"- {error}\n"
        
        if statistical_validation.get("warnings"):
            report += "**‚ö†Ô∏è Alertas:**\n"
            for warning in statistical_validation["warnings"]:
                report += f"- {warning}\n"
        
        report += f"""

### Recomenda√ß√µes Priorit√°rias
"""
        
        # Combinar recomenda√ß√µes de todas as avalia√ß√µes
        all_recommendations = []
        all_recommendations.extend(quality_assessment.get("recommendations", []))
        all_recommendations.extend(prisma_compliance.get("recommendations", []))
        all_recommendations.extend(statistical_validation.get("recommendations", []))
        
        for i, rec in enumerate(all_recommendations[:10], 1):  # Top 10 recomenda√ß√µes
            report += f"{i}. {rec}\n"
        
        report += f"""

### Pr√≥ximos Passos
{'‚úÖ Relat√≥rio aprovado para finaliza√ß√£o' if quality_assessment.get('overall_score', 0) >= 80 and statistical_validation.get('is_valid', False) else 'üìù Revis√µes necess√°rias antes da finaliza√ß√£o'}

---
*Relat√≥rio gerado em {datetime.now().strftime('%d/%m/%Y √†s %H:%M')}*
"""
        
        return report.strip()
        
    except Exception as e:
        return f"Erro na gera√ß√£o do relat√≥rio de qualidade: {str(e)}"